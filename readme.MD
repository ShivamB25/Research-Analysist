
![alt text](sources/33c2bee189115cebc6f1e5104702a11b.png)
# Research-Analysist

Welcome to the Research-Analysist project! This project is designed to automate the process of web scraping and report generation, providing comprehensive research reports based on user-defined topics.

## Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
- [Features](#features)
- [Installation](#installation)
- [Usage](#usage)
- [Detailed Workflow](#detailed-workflow)
- [Agents and Their Roles](#agents-and-their-roles)
- [Tasks and Their Execution](#tasks-and-their-execution)
- [Contributing](#contributing)
- [License](#license)
- [Contact](#contact)

## Overview

The Research-Analysist project is divided into two main components:
1. **Web Scraping and Data Collection**: Gathers data from the web based on user-defined topics.
2. **Report Generation**: Generates comprehensive research reports based on the collected data.

## Architecture
![alt text](sources/design.png)
### Web Scraping and Data Collection

- **Agents**: Handle specific tasks such as searching the web, scraping content, and generating follow-up questions.
- **Data Manager**: Manages the collected data, ensuring it is stored, processed, and saved correctly.
- **Main Script**: Orchestrates the agents and manages the workflow.

### Report Generation

- **Agents**: Handle tasks such as generating an index of topics, creating summaries, and writing detailed content.
- **Tasks**: Specific tasks assigned to agents to perform their roles.
- **Main Script**: Orchestrates the agents and tasks, managing the workflow.

## Features

- **Automated Web Scraping**: Collects data from the web based on user-defined topics.
- **Intelligent Agents**: Uses language models to perform context-aware processing.
- **Comprehensive Reports**: Generates detailed research reports with summaries, indexes, and in-depth content.
- **Asynchronous Processing**: Efficiently handles web scraping tasks.
- **Microservices Architecture**: Components can be run independently or as microservices.

## Installation

### Prerequisites

- Python 3.8 or higher
- [pip](https://pip.pypa.io/en/stable/)

### Steps

1. **Clone the Repository**:
   ```sh
   git clone https://github.com/ShivamB25/Research-Analysist
   cd Research-Analysist
   ```

2. **Install Dependencies**:
   ```sh
   pip install -r requirements.txt
   ```

3. **Set Up Environment Variables**:
   - Copy the `.env.example` file to `.env` and fill in the required values.

## Usage

### Web Scraping and Data Collection

1. **Configure the Main Query**:
   - Open `web/main.py` and set the `main_query` variable to your topic of interest.
   ```python
   main_query = "decentralized GPU network"  # Replace with your topic
   ```

2. **Run the Script**:
   ```sh
   python web/main.py
   ```

### Report Generation

1. **Ensure Scraped Data is Available**:
   - Make sure `scraped_data.json` is generated by the web scraping component.

2. **Run the Report Generation Script**:
   ```sh
   python data_to_report/main.py
   ```

## Detailed Workflow

### Web Scraping and Data Collection

1. **Initialization**:
   - Load environment variables and initialize agents.
   - Users input their research topic in the `main_query` variable.

2. **Main Query**:
   - Perform an initial search using the `SearchAgent`.
   - Retrieve top search results and extract URLs.

3. **Follow-Up Questions**:
   - Generate 10 follow-up questions based on the main query.
   - Perform searches for each follow-up question and retrieve top results.

4. **Scraping Content**:
   - Asynchronously scrape content from the collected URLs using the `ScraperAgent`.
   - Store the scraped content in `scraped_data.json`.

5. **Data Management**:
   - Manage the scraped content using the `DataManager`.

### Report Generation

1. **Initialization**:
   - Load environment variables and initialize agents and tasks.

2. **Reading Scraped Data**:
   - Load the scraped data from `scraped_data.json`.

3. **Generating Summary**:
   - Generate a brief summary of the research data using the `SummaryAgent`.

4. **Generating Index**:
   - Generate a structured index of topics and subtopics using the `IndexAgent`.

5. **Generating Detailed Content**:
   - Generate detailed content for each main topic and subtopic using the `DetailAgent`.

6. **Creating LaTeX Document**:
   - Compile the generated content into a LaTeX document.

7. **Generating PDF Report**:
   - Convert the LaTeX document into a PDF report and upload it to an online platform.

## Agents and Their Roles

### Web Scraping and Data Collection Agents

1. **SearchAgent**:
   - Performs web searches and generates follow-up questions.
   - Uses the `EnhancedSerperDevTool` for performing searches.

2. **ScraperAgent**:
   - Extracts content from web pages asynchronously.
   - Uses the `WebScraper` tool for scraping content.

3. **ContextGeneratorAgent** (Not actively used in the current implementation):
   - Synthesizes information from multiple sources and generates insightful analysis.

### Report Generation Agents

1. **IndexAgent**:
   - Generates a structured index of topics and subtopics.

2. **SummaryAgent**:
   - Creates brief summaries for main topics.

3. **DetailAgent**:
   - Produces in-depth content for subtopics.

## Tasks and Their Execution

### Web Scraping and Data Collection Tasks

1. **InitialSearchTask**:
   - Performs an internet search for the main query and collects the top 5 results.

2. **FollowUpSearchTask**:
   - Performs an internet search for a follow-up question and collects the top 5 results.

3. **ScrapingTasks**:
   - Scrapes content from the given URLs and returns the text.

### Report Generation Tasks

1. **ContextTask**:
   - Generates a structured index of main topics and subtopics based on the research data.

2. **SummaryTask**:
   - Generates a brief and concise summary for the main topic, including the subtopics.

3. **DetailTask**:
   - Produces detailed content for a subtopic on the main topic.

## Contributing

We welcome contributions! Please follow these steps:

1. Fork the repository.
2. Create a new branch (`git checkout -b feature-branch`).
3. Make your changes and commit them (`git commit -m 'Add new feature'`).
4. Push to the branch (`git push origin feature-branch`).
5. Create a pull request.

## License

This project is licensed under the MIT License. See the [LICENSE.md](LICENSE.md) file for details.

## Contact

For any questions or feedback, please reach out to [Shiambansal.in30@gmail.com](mailto:Shiambansal.in30@gmail.com).

